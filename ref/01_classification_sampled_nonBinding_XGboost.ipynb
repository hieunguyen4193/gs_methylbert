{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20d1f9-8091-4879-9247-17edb2039427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from optimization import *\n",
    "from configuration_bert import *\n",
    "from tokenization_bert import *\n",
    "from modeling_bert import *\n",
    "from tokenization_dna import *\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from preparation import *\n",
    "from helper_functions import *\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "MASK_LIST = [-1, 1, 2]\n",
    "\n",
    "modeldir = \"Diem_pretrained_model\"\n",
    "datadir = \"dataset\"\n",
    "\n",
    "outdir = \".\"\n",
    "\n",
    "path_to_main_output = os.path.join(outdir, \"outputdir\")\n",
    "os.system(\"mkdir -p {}\".format(path_to_main_output))\n",
    "\n",
    "path_to_01_output = os.path.join(path_to_main_output, \"01_output\")\n",
    "os.system(\"mkdir -p {}\".format(path_to_01_output))\n",
    "\n",
    "model = BertModel.from_pretrained(os.path.join(modeldir, \"23082023_checkpoints\", \"checkpoint-9900\"))\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c133c-104c-4016-a92b-61f22fd67961",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(path_to_01_output, \"universal_testset_new.stringOnly.label.txt\")) == False:    \n",
    "    df_full = pd.read_csv(os.path.join(datadir, \"train_fullpeps.csv\"))\n",
    "    df_test = pd.read_csv(os.path.join(datadir, \"universal_testset_new.csv\"))\n",
    "    \n",
    "    df_full[\"string\"] = df_full[[\"CDR3b\", \"epitope\"]].apply(lambda x: \"{} [SEP] {}\".format(\" \".join([item for item in x[0]]), \n",
    "                                                                                              \" \".join([item for item in x[1]])), axis = 1)\n",
    "    df_binding = df_full[df_full[\"binder\"] == 1]\n",
    "    df_nonbinding = df_full[df_full[\"binder\"] == 0]\n",
    "    \n",
    "    df_binding[\"string\"].to_csv(os.path.join(path_to_01_output, \"train_fullpeps.binding.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    df_nonbinding[\"string\"].to_csv(os.path.join(path_to_01_output, \"train_fullpeps.nonbinding.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    \n",
    "    os.system(\"mkdir -p {}\".format(os.path.join(path_to_01_output, \"sampled_non_binding\")))\n",
    "    for i in tqdm(range(1,101)):\n",
    "        tmp = df_nonbinding.sample(10000)[\"string\"].to_csv(os.path.join(path_to_01_output, \"sampled_non_binding\", \"train_fullpeps.nonbinding.stringOnly.sample_{}.txt\".format(i)), \n",
    "                                                          sep = \"\\t\", header = False, index = False)\n",
    "    \n",
    "    df_test[\"string\"] = df_test[[\"CDR3b\", \"epitope\"]].apply(lambda x: \"{} [SEP] {}\".format(\" \".join([item for item in x[0]]), \n",
    "                                                                                              \" \".join([item for item in x[1]])), axis = 1)\n",
    "    \n",
    "    df_test[\"string\"].to_csv(os.path.join(path_to_01_output, \"universal_testset_new.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    df_test[\"binder\"].to_csv(os.path.join(path_to_01_output, \"universal_testset_new.stringOnly.label.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c07b5-e4b1-4d3f-9caa-f100f60c096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRANSFORM THE INPUT SEQUENCES TO [CLS] EMBEDDING VECTORS 1x1024\n",
    "path_to_binding_dataset = os.path.join(path_to_01_output, \"train_fullpeps.binding.stringOnly.txt\")\n",
    "batch_size = 32\n",
    "\n",
    "cls_embeddings_binding = generate_embedding_matrix(path_to_binding_dataset, batch_size, model, tokenizer)\n",
    "\n",
    "sample_id = 1\n",
    "path_to_nonbinding_dataset = os.path.join(path_to_01_output, \"sampled_non_binding\", \"train_fullpeps.nonbinding.stringOnly.sample_{}.txt\".format(sample_id))\n",
    "cls_embeddings_nonbinding = generate_embedding_matrix(path_to_nonbinding_dataset, batch_size, model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29567d7b-4321-4aca-8b4d-41a7e38a5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([cls_embeddings_binding, cls_embeddings_nonbinding])\n",
    "y = np.array([1 for i in range(cls_embeddings_binding.shape[0])] + [0 for i in range(cls_embeddings_nonbinding.shape[0])])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4b40e-2940-4231-ba47-570db20dd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "bst = XGBClassifier(n_estimators=100, max_depth=100, learning_rate=0.01, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "preds = bst.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74083bc-e917-4b49-849a-002406ca1d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "universal_test_acc = []\n",
    "finished_split_name = []\n",
    "all_sens = []\n",
    "all_spec = []\n",
    "\n",
    "all_num_binding = []\n",
    "all_num_nonbinding = []\n",
    "\n",
    "path_to_universal_test_dir = os.path.join(datadir, \"split_universal_testset\")\n",
    "\n",
    "all_split_names = [item.name for item in pathlib.Path(path_to_universal_test_dir).glob(\"*\")]\n",
    "\n",
    "path_to_save_predictiondf = \"{}/prediction_results_with_nonBinding_sampleID_{}\".format(path_to_01_output, sample_id)\n",
    "os.system(\"mkdir -p {}\".format(path_to_save_predictiondf))\n",
    "\n",
    "done_predictiondf = [item.name.replace(\"predictiondf_\", \"\").replace(\".csv\", \"\") for item in pathlib.Path(path_to_save_predictiondf).glob(\"*.csv\")]\n",
    "for split_name in all_split_names:\n",
    "    if (split_name in done_predictiondf) == False:\n",
    "        path_to_universal_test = os.path.join(datadir, \"split_universal_testset\", split_name)\n",
    "        cls_embeddings_univervsal_test = generate_embedding_matrix(path_to_universal_test, batch_size, model, tokenizer)\n",
    "        \n",
    "        path_to_universal_test_labels = os.path.join(datadir, \"split_universal_testset_labels\", split_name)\n",
    "        universal_test_labels = pd.read_csv(path_to_universal_test_labels, header = None)[0].to_numpy()\n",
    "        \n",
    "        test_preds = bst.predict(cls_embeddings_univervsal_test)\n",
    "        test_probs = bst.predict_proba(cls_embeddings_univervsal_test)\n",
    "        predictiondf = pd.DataFrame(data = universal_test_labels, columns = [\"true_label\"])\n",
    "        predictiondf[\"prediction\"] = test_preds\n",
    "        acc = accuracy_score(universal_test_labels, test_preds)\n",
    "        if predictiondf[predictiondf[\"true_label\"] == 1].shape[0] == 0:\n",
    "            tp = \"no binding sample\"\n",
    "            fp = predictiondf[(predictiondf[\"true_label\"] == 0) & (predictiondf[\"prediction\"] == 1)].shape[0]/predictiondf[predictiondf[\"true_label\"] == 0].shape[0]\n",
    "        elif predictiondf[predictiondf[\"true_label\"] == 0].shape[0] == 0:\n",
    "            tp = predictiondf[(predictiondf[\"true_label\"] == 1) & (predictiondf[\"prediction\"] == 1)].shape[0]/predictiondf[predictiondf[\"true_label\"] == 1].shape[0]\n",
    "            fp = \"no nonbinding sample\"\n",
    "        else:\n",
    "            fp = predictiondf[(predictiondf[\"true_label\"] == 0) & (predictiondf[\"prediction\"] == 1)].shape[0]/predictiondf[predictiondf[\"true_label\"] == 0].shape[0]\n",
    "            tp = predictiondf[(predictiondf[\"true_label\"] == 1) & (predictiondf[\"prediction\"] == 1)].shape[0]/predictiondf[predictiondf[\"true_label\"] == 1].shape[0]\n",
    "        \n",
    "        spec = 1 - fp\n",
    "        sens = tp\n",
    "        \n",
    "        universal_test_acc.append(acc)\n",
    "        finished_split_name.append(split_name)\n",
    "        all_sens.append(sens)\n",
    "        all_spec.append(spec)\n",
    "        all_num_binding.append(predictiondf[predictiondf[\"true_label\"] == 1].shape[0])\n",
    "        all_num_nonbinding.append(predictiondf[predictiondf[\"true_label\"] == 0].shape[0])\n",
    "        predictiondf[\"string\"] = pd.read_csv(path_to_universal_test, header = None)[0].to_list()\n",
    "        predictiondf[\"prob_0\"] = test_probs[:, 0]\n",
    "        predictiondf[\"prob_1\"] = test_probs[:, 1]\n",
    "        predictiondf.to_csv(os.path.join(path_to_save_predictiondf, \"predictiondf_{}.csv\".format(split_name)))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
