{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58177be3-b816-4039-ada9-048c1b02ed70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:loading configuration file /mnt/WORKDIR/hieunguyen/outdir/TCR_antigen_binding_transformers/pytorch/model_files/models/bert-large-cased-config.json\n",
      "INFO:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 27\n",
      "}\n",
      "\n",
      "INFO:loading configuration file Diem_pretrained_model/23082023_checkpoints/checkpoint-9900/config.json\n",
      "INFO:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"num_rnn_layer\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.0,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"split\": 10,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 27\n",
      "}\n",
      "\n",
      "INFO:loading weights file Diem_pretrained_model/23082023_checkpoints/checkpoint-9900/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from optimization import *\n",
    "from configuration_bert import *\n",
    "from tokenization_bert import *\n",
    "from modeling_bert import *\n",
    "from tokenization_dna import *\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from preparation import *\n",
    "from helper_functions import *\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_process = 1\n",
    "\n",
    "MASK_LIST = [-1, 1, 2]\n",
    "\n",
    "modeldir = \"Diem_pretrained_model\"\n",
    "datadir = \"dataset\"\n",
    "\n",
    "outdir = \".\"\n",
    "\n",
    "path_to_main_output = os.path.join(outdir, \"outputdir\")\n",
    "os.system(\"mkdir -p {}\".format(path_to_main_output))\n",
    "\n",
    "path_to_02_output = os.path.join(path_to_main_output, \"02_output\")\n",
    "os.system(\"mkdir -p {}\".format(path_to_02_output))\n",
    "path_to_01_output = os.path.join(path_to_main_output, \"01_output\")\n",
    "\n",
    "model = BertModel.from_pretrained(os.path.join(modeldir, \"23082023_checkpoints\", \"checkpoint-9900\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7fad0f-7a5a-4ca8-ab0d-618f829c9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(path_to_02_output, \"universal_testset_new.stringOnly.label.txt\")) == False:    \n",
    "    df_full = pd.read_csv(os.path.join(datadir, \"train_fullpeps.csv\"))\n",
    "    df_test = pd.read_csv(os.path.join(datadir, \"universal_testset_new.csv\"))\n",
    "    \n",
    "    df_full[\"string\"] = df_full[[\"CDR3b\", \"epitope\"]].apply(lambda x: \"{} [SEP] {}\".format(\" \".join([item for item in x[0]]), \n",
    "                                                                                              \" \".join([item for item in x[1]])), axis = 1)\n",
    "    df_binding = df_full[df_full[\"binder\"] == 1]\n",
    "    df_nonbinding = df_full[df_full[\"binder\"] == 0]\n",
    "    \n",
    "    df_binding[\"string\"].to_csv(os.path.join(path_to_02_output, \"train_fullpeps.binding.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    df_nonbinding[\"string\"].to_csv(os.path.join(path_to_02_output, \"train_fullpeps.nonbinding.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "        \n",
    "    df_test[\"string\"] = df_test[[\"CDR3b\", \"epitope\"]].apply(lambda x: \"{} [SEP] {}\".format(\" \".join([item for item in x[0]]), \n",
    "                                                                                              \" \".join([item for item in x[1]])), axis = 1)\n",
    "    \n",
    "    df_test[\"string\"].to_csv(os.path.join(path_to_02_output, \"universal_testset_new.stringOnly.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    df_test[\"binder\"].to_csv(os.path.join(path_to_02_output, \"universal_testset_new.stringOnly.label.txt\"), sep = \"\\t\", header = False, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4cf7f2f-543e-4579-b79c-34edce82ab01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####----------------------------------------------------------------#####\n",
    "##### GENERATE .CSV FILE FOR NON-BINDING EMBEDDING VECTORS. \n",
    "#####----------------------------------------------------------------#####\n",
    "if os.path.exists(os.path.join(path_to_02_output, \"finished_generating_cls_embeddings_nonBinding.txt\")) == False:\n",
    "    batch_size = 32\n",
    "    path_to_input_dataset = os.path.join(path_to_02_output, \"train_fullpeps.nonbinding.stringOnly.txt\")\n",
    "    input_dataset = load_and_cache_examples(path_to_input_dataset, tokenizer, n_process = n_process)\n",
    "    transform_dataloader = DataLoader(input_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "    cls_embeddings = np.empty((0, 1024))\n",
    "    epoch_iterator = tqdm(transform_dataloader, desc=\"Iteration\")\n",
    "    \n",
    "    finished_batch = pd.read_csv(os.path.join(path_to_02_output, \"finished_batch_cls_embeddings_nonBinding.txt\"))\n",
    "    embedding_outputfile = os.path.join(path_to_02_output, \"cls_embeddings_nonBinding.csv\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        if step not in finished_batch[\"step\"].to_list():\n",
    "            batch = batch.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(batch)\n",
    "            last_hidden_states = outputs[0]  \n",
    "            tmp_cls_embeddings = last_hidden_states[:, 0, :]\n",
    "            tmp_cls_embeddings = tmp_cls_embeddings.cpu().detach().numpy()\n",
    "            pd.DataFrame(tmp_cls_embeddings).to_csv(embedding_outputfile, mode = \"a\", header = False)\n",
    "            os.system(\"echo {} >> {}\".format(step, \n",
    "                                             os.path.join(path_to_02_output, \"finished_batch_cls_embeddings_nonBinding.txt\")))\n",
    "    os.system(\"touch {}/finished_generating_cls_embeddings_nonBinding.txt\".format(path_to_02_output))\n",
    "\n",
    "#####----------------------------------------------------------------#####\n",
    "##### GENERATE .CSV FILE FOR BINDING EMBEDDING VECTORS\n",
    "#####----------------------------------------------------------------#####\n",
    "if os.path.exists(os.path.join(path_to_02_output, \"finished_generating_cls_embeddings_Binding.txt\")) == False:\n",
    "    batch_size = 32\n",
    "    path_to_input_dataset = os.path.join(path_to_02_output, \"train_fullpeps.binding.stringOnly.txt\")\n",
    "    input_dataset = load_and_cache_examples(path_to_input_dataset, tokenizer, n_process = n_process)\n",
    "    transform_dataloader = DataLoader(input_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "    cls_embeddings = np.empty((0, 1024))\n",
    "    epoch_iterator = tqdm(transform_dataloader, desc=\"Iteration\")\n",
    "    \n",
    "    finished_batch = pd.read_csv(os.path.join(path_to_02_output, \"finished_batch_cls_embeddings_Binding.txt\"))\n",
    "    embedding_outputfile = os.path.join(path_to_02_output, \"cls_embeddings_Binding.csv\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        if step not in finished_batch[\"step\"].to_list():\n",
    "            batch = batch.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(batch)\n",
    "            last_hidden_states = outputs[0]  \n",
    "            tmp_cls_embeddings = last_hidden_states[:, 0, :]\n",
    "            tmp_cls_embeddings = tmp_cls_embeddings.cpu().detach().numpy()\n",
    "            pd.DataFrame(tmp_cls_embeddings).to_csv(embedding_outputfile, mode = \"a\", header = False)\n",
    "            os.system(\"echo {} >> {}\".format(step, \n",
    "                                             os.path.join(path_to_02_output, \"finished_batch_cls_embeddings_Binding.txt\")))\n",
    "    \n",
    "    os.system(\"touch {}/finished_generating_cls_embeddings_Binding.txt\".format(path_to_02_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a6713b-86af-4de8-8dfa-67917b931617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cls(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d7851-936c-4268-ac52-bfa2eedf4be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
